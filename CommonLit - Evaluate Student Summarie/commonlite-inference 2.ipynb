{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4e1a0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-11T16:04:08.227891Z",
     "iopub.status.busy": "2023-10-11T16:04:08.227580Z",
     "iopub.status.idle": "2023-10-11T16:05:18.007941Z",
     "shell.execute_reply": "2023-10-11T16:05:18.006703Z",
     "shell.execute_reply.started": "2023-10-11T16:04:08.227864Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9c482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:18.010447Z",
     "iopub.status.busy": "2023-10-11T16:05:18.010150Z",
     "iopub.status.idle": "2023-10-11T16:05:46.216085Z",
     "shell.execute_reply": "2023-10-11T16:05:46.214984Z",
     "shell.execute_reply.started": "2023-10-11T16:05:18.010421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModelForMultipleChoice\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5858481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:46.220089Z",
     "iopub.status.busy": "2023-10-11T16:05:46.219887Z",
     "iopub.status.idle": "2023-10-11T16:05:55.656592Z",
     "shell.execute_reply": "2023-10-11T16:05:55.655731Z",
     "shell.execute_reply.started": "2023-10-11T16:05:46.220062Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "google_electra_base = pd.read_csv('/kaggle/input/google-electra-commonlite-student-summury/train.csv')\n",
    "longformer_large_4096 = pd.read_csv('/kaggle/input/longformer-large-4096-commonlite-student-summury/train.csv')\n",
    "\n",
    "bert_large_content  = pd.read_csv('/kaggle/input/bert-large-commonlite-student-summury-content/train.csv')\n",
    "\n",
    "reberta_large = pd.read_csv('/kaggle/input/reberta-large-commonlite-student-summury/train.csv')\n",
    "reberta_base = pd.read_csv('/kaggle/input/reberta-commonlite-student-summury-traning/train.csv')\n",
    "\n",
    "deberta_base_wording = pd.read_csv('/kaggle/input/debert-best-public-v3-base-content/train.csv')\n",
    "deberta_base_content = pd.read_csv('/kaggle/input/debert-best-public-v3-base-wording/train.csv')\n",
    "\n",
    "deberta_large_wording = pd.read_csv('/kaggle/input/deberta-large-wording-commonlite-student-summury-t/train.csv')\n",
    "deberta_large_content4fold = pd.read_csv('/kaggle/input/deberta-v3-large-content-4fold-training-data/train.csv')\n",
    "deberta_large_content2fold = pd.read_csv('/kaggle/input/deberta-v3-large-content-2fold-training-data/train.csv')\n",
    "deperta_small = pd.read_csv('/kaggle/input/deberta-smal-of-commonlite-student-summury-traning/train.csv')\n",
    "\n",
    "debertav3base_freeze6_layers = pd.read_csv('/kaggle/input/deberta-v3-base-freeze-6-layers/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d0812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:55.659480Z",
     "iopub.status.busy": "2023-10-11T16:05:55.658976Z",
     "iopub.status.idle": "2023-10-11T16:05:55.672809Z",
     "shell.execute_reply": "2023-10-11T16:05:55.671991Z",
     "shell.execute_reply.started": "2023-10-11T16:05:55.659445Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82796c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:55.674551Z",
     "iopub.status.busy": "2023-10-11T16:05:55.674310Z",
     "iopub.status.idle": "2023-10-11T16:05:55.680738Z",
     "shell.execute_reply": "2023-10-11T16:05:55.679224Z",
     "shell.execute_reply.started": "2023-10-11T16:05:55.674520Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name=\"debertav3base\" # roberta-base\n",
    "    learning_rate=1.5e-5\n",
    "    weight_decay=0.02\n",
    "    hidden_dropout_prob=0.00\n",
    "    attention_probs_dropout_prob=0.00\n",
    "    num_train_epochs=5\n",
    "    n_splits=4\n",
    "    batch_size=12\n",
    "    random_seed=42\n",
    "    save_steps=100\n",
    "    max_length=512\n",
    "    test_max_length=384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096f99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:55.682914Z",
     "iopub.status.busy": "2023-10-11T16:05:55.682740Z",
     "iopub.status.idle": "2023-10-11T16:05:55.808168Z",
     "shell.execute_reply": "2023-10-11T16:05:55.807305Z",
     "shell.execute_reply.started": "2023-10-11T16:05:55.682884Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "\n",
    "prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n",
    "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
    "\n",
    "summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")#[4:10]\n",
    "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54ad5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:55.810160Z",
     "iopub.status.busy": "2023-10-11T16:05:55.809604Z",
     "iopub.status.idle": "2023-10-11T16:05:57.731702Z",
     "shell.execute_reply": "2023-10-11T16:05:57.730716Z",
     "shell.execute_reply.started": "2023-10-11T16:05:55.810127Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                ) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        self.speller = Speller(lang='en')\n",
    "        self.spellchecker = SpellChecker() \n",
    "        \n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        # Use the zip function to help us generate n-grams\n",
    "        # Concatentate the tokens into ngrams and return\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int) -> int:\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n",
    "        self.spellchecker.word_frequency.load_words(tokens)\n",
    "        self.speller.nlp_data.update({token:1000 for token in tokens})\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        \n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        \n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "        \n",
    "        # Add prompt tokens into spelling checker dictionary\n",
    "        prompts[\"prompt_tokens\"].apply(\n",
    "            lambda x: self.add_spelling_dictionary(x)\n",
    "        )\n",
    "        \n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        # fix misspelling\n",
    "        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n",
    "            lambda x: self.speller(x)\n",
    "        )\n",
    "        \n",
    "        # count misspelling\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "        \n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # after merge preprocess\n",
    "#         input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n",
    "        \n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        \n",
    "        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor(model_name=CFG.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c77dfd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0704d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.734184Z",
     "iopub.status.busy": "2023-10-11T16:05:57.733058Z",
     "iopub.status.idle": "2023-10-11T16:05:57.756473Z",
     "shell.execute_reply": "2023-10-11T16:05:57.755584Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.734013Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_this = ['content_pred_longformer-large-4096',\n",
    "       'wording_pred_longformer-large-4096']\n",
    "\n",
    "# please comment this if you submite to competetion\n",
    "train = longformer_large_4096.drop(drop_this, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce18e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.758550Z",
     "iopub.status.busy": "2023-10-11T16:05:57.758297Z",
     "iopub.status.idle": "2023-10-11T16:05:57.885547Z",
     "shell.execute_reply": "2023-10-11T16:05:57.884560Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.758518Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\n",
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2690c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.889331Z",
     "iopub.status.busy": "2023-10-11T16:05:57.889110Z",
     "iopub.status.idle": "2023-10-11T16:05:57.894318Z",
     "shell.execute_reply": "2023-10-11T16:05:57.893314Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.889305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gkf = GroupKFold(n_splits = CFG.n_splits)\n",
    "\n",
    "# for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
    "#     train.loc[val_index, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69475a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.896638Z",
     "iopub.status.busy": "2023-10-11T16:05:57.895751Z",
     "iopub.status.idle": "2023-10-11T16:05:57.910364Z",
     "shell.execute_reply": "2023-10-11T16:05:57.909167Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.896601Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48fa4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.912094Z",
     "iopub.status.busy": "2023-10-11T16:05:57.911846Z",
     "iopub.status.idle": "2023-10-11T16:05:57.922703Z",
     "shell.execute_reply": "2023-10-11T16:05:57.921585Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.912062Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f3fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.925458Z",
     "iopub.status.busy": "2023-10-11T16:05:57.924698Z",
     "iopub.status.idle": "2023-10-11T16:05:57.934058Z",
     "shell.execute_reply": "2023-10-11T16:05:57.933078Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.925424Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.936351Z",
     "iopub.status.busy": "2023-10-11T16:05:57.935816Z",
     "iopub.status.idle": "2023-10-11T16:05:57.946141Z",
     "shell.execute_reply": "2023-10-11T16:05:57.945296Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.936311Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd891d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.948197Z",
     "iopub.status.busy": "2023-10-11T16:05:57.947868Z",
     "iopub.status.idle": "2023-10-11T16:05:57.972699Z",
     "shell.execute_reply": "2023-10-11T16:05:57.971628Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.948166Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentScoreRegressor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                model_dir: str,\n",
    "                target: str,\n",
    "                hidden_dropout_prob: float,\n",
    "                attention_probs_dropout_prob: float,\n",
    "                max_length: int,\n",
    "                test_max_length: int,\n",
    "                ):\n",
    "        \n",
    "        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n",
    "        self.input_col = \"input\"\n",
    "        \n",
    "        self.text_cols = [self.input_col] \n",
    "        self.target = target\n",
    "        self.target_cols = [target]\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = model_dir\n",
    "        self.max_length = max_length\n",
    "        self.test_max_length = test_max_length\n",
    "        \n",
    "            \n",
    "        if model_name == 'google_electra_base':\n",
    "            model_name = 'transformers/google-electra-base-discriminator'\n",
    "            \n",
    "        if model_name == 'deperta_small':\n",
    "            model_name = 'huggingfacedebertav3variants/deberta-v3-small'\n",
    "        \n",
    "        if model_name == 'deberta_base_wording':\n",
    "            model_name = 'debertav3base'\n",
    "            \n",
    "        if model_name == 'deberta_base_content':\n",
    "            model_name = 'debertav3base'\n",
    "            \n",
    "        if model =='debertav3base_freeze6_layers':\n",
    "            model_name = 'debertav3base'\n",
    "            \n",
    "        if model_name == 'deberta_large_wording':\n",
    "            model_name = 'microsoft-deberta-v3-large'\n",
    "            \n",
    "        if model_name == 'deberta_large_content2fold':\n",
    "            model_name = 'microsoft-deberta-v3-large'\n",
    "            \n",
    "        if model_name =='deberta_large_content4fold':\n",
    "            model_name = 'microsoft-deberta-v3-large'\n",
    "            \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        \n",
    "        self.model_config.update({\n",
    "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "            \"num_labels\": 1,\n",
    "            \"problem_type\": \"regression\",\n",
    "        })\n",
    "        \n",
    "        seed_everything(seed=42)\n",
    "\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "\n",
    "    def tokenize_function(self, examples: pd.DataFrame):\n",
    "        labels = [examples[self.target]]\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.test_max_length)\n",
    "        return tokenized\n",
    "        \n",
    "    def train(self, \n",
    "            fold: int,\n",
    "            train_df: pd.DataFrame,\n",
    "            valid_df: pd.DataFrame,\n",
    "            batch_size: int,\n",
    "            learning_rate: float,\n",
    "            weight_decay: float,\n",
    "            num_train_epochs: float,\n",
    "            save_steps: int,\n",
    "        ) -> None:\n",
    "        \"\"\"fine-tuning\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        train_df[self.input_col] = (\n",
    "                    train_df[\"prompt_title\"] + sep \n",
    "                    + train_df[\"prompt_question\"] + sep \n",
    "                    + train_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "\n",
    "        valid_df[self.input_col] = (\n",
    "                    valid_df[\"prompt_title\"] + sep \n",
    "                    + valid_df[\"prompt_question\"] + sep \n",
    "                    + valid_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        \n",
    "        train_df = train_df[[self.input_col] + self.target_cols]\n",
    "        valid_df = valid_df[[self.input_col] + self.target_cols]\n",
    "        \n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(\n",
    "            f\"/kaggle/input/{self.model_name}\", \n",
    "            config=self.model_config\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n",
    "        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n",
    "    \n",
    "        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n",
    "        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n",
    "\n",
    "        # eg. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            load_best_model_at_end=True, # select best model\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            report_to='none',\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=save_steps,\n",
    "            save_steps=save_steps,\n",
    "            metric_for_best_model=\"rmse\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_content,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized_datasets,\n",
    "            eval_dataset=val_tokenized_datasets,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        model_content.save_pretrained(self.model_dir)\n",
    "        self.tokenizer.save_pretrained(self.model_dir)\n",
    "\n",
    "        \n",
    "    def predict(self, \n",
    "                test_df: pd.DataFrame,\n",
    "                fold: int,\n",
    "               ):\n",
    "        \"\"\"predict content score\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        in_text = (\n",
    "                    test_df[\"prompt_title\"] + sep \n",
    "                    + test_df[\"prompt_question\"] + sep \n",
    "                    + test_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        test_df[self.input_col] = in_text\n",
    "\n",
    "        test_ = test_df[[self.input_col]]\n",
    "    \n",
    "        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n",
    "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
    "\n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n",
    "        model_content.eval()\n",
    "        \n",
    "        # e.g. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "\n",
    "        \n",
    "        test_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size = 4,   \n",
    "            dataloader_drop_last = False,\n",
    "        )\n",
    "\n",
    "        # init trainer\n",
    "        infer_content = Trainer(\n",
    "                      model = model_content, \n",
    "                      tokenizer=self.tokenizer,\n",
    "                      data_collator=self.data_collator,\n",
    "                      args = test_args)\n",
    "\n",
    "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cd43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:05:57.975572Z",
     "iopub.status.busy": "2023-10-11T16:05:57.975222Z",
     "iopub.status.idle": "2023-10-11T16:05:57.990572Z",
     "shell.execute_reply": "2023-10-11T16:05:57.989755Z",
     "shell.execute_reply.started": "2023-10-11T16:05:57.975549Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "    test_df: pd.DataFrame,\n",
    "    target:str,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length : int,\n",
    "    test_max_length:int,\n",
    "    ):\n",
    "    \"\"\"predict using mean folds\"\"\"\n",
    "\n",
    "    tfc = 0\n",
    "    if model_name == 'deberta_large_content2fold' : tfc =2\n",
    "    \n",
    "    for fold in range(CFG.n_splits - tfc):\n",
    "        print(f\"fold {fold} =>\", end='')\n",
    "        \n",
    "        if model =='debertav3base_freeze6_layers':\n",
    "            model_dir = f'/kaggle/input/deberta-v3-base-freeze-6layer-commonlite/{target}/debertav3base/fold_{fold}'\n",
    "        \n",
    "        if model_name == 'google_electra_base':\n",
    "            model_dir = f'/kaggle/input/commonlitegoogle-electra-base-discriminator/{target}/google-electra-base-discriminator/fold_{fold}'\n",
    "        \n",
    "        if model_name == 'deperta_small':\n",
    "            model_dir = f'/kaggle/input/commonlitedepertasmall/{target}/debertav3small/fold_{fold}'\n",
    "            \n",
    "        if model_name == 'deberta_base_wording':\n",
    "            model_dir = f'/kaggle/input/commonlitedebertav3base-wording/wording/debertav3base/fold_{fold}'\n",
    "            \n",
    "        if model_name == 'deberta_base_content':\n",
    "            model_dir = f'/kaggle/input/commonlitedebertav3base-content/content/debertav3base/fold_{fold}'\n",
    "            \n",
    "        if model_name == 'deberta_large_wording':\n",
    "            model_dir = f'/kaggle/input/commonlite-deperta-large-wording/wording/microsoft-deberta-v3-large/fold_{fold}'\n",
    "            \n",
    "        if model_name == 'deberta_large_content2fold':\n",
    "            model_dir = f'/kaggle/input/commonlitemicrosoft-deberta-v3-large-content-2fold/content/microsoft-deberta-v3-large/fold_{fold}'\n",
    "            \n",
    "        if model_name == 'deberta_large_content4fold':\n",
    "            model_dir = f'/kaggle/input/commonlitemicrosoft-deberta-v3-large/content/microsoft-deberta-v3-large/fold_{fold}'\n",
    "            \n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "            test_max_length=test_max_length,\n",
    "           )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=test_df,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        test_df[f\"{target}_pred_{fold}\"] = pred\n",
    "    \n",
    "    test_df[f\"{target}_{model_name}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits - tfc)]].mean(axis=1)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb69c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Stacking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd987b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:06:06.710582Z",
     "iopub.status.busy": "2023-10-11T16:06:06.710318Z",
     "iopub.status.idle": "2023-10-11T16:06:06.715670Z",
     "shell.execute_reply": "2023-10-11T16:06:06.714683Z",
     "shell.execute_reply.started": "2023-10-11T16:06:06.710554Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NLP_MODEL = [\n",
    "    \n",
    "    'deberta_large_content4fold',\n",
    "    'deberta_large_wording',\n",
    "    \n",
    "    \n",
    "    'debertav3base_freeze6_layers',\n",
    "    \n",
    "    'google_electra_base',\n",
    "#     'longformer_large_4096',\n",
    "#     'reberta_large',\n",
    "#     'reberta_base',\n",
    "    \n",
    "    'deberta_base_wording',\n",
    "    'deberta_base_content',\n",
    "    \n",
    "#     'deberta_large_content2fold',\n",
    "#     'deperta_small',\n",
    "#     'bert_large_content',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ee021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:06:07.990926Z",
     "iopub.status.busy": "2023-10-11T16:06:07.989924Z",
     "iopub.status.idle": "2023-10-11T16:06:07.995848Z",
     "shell.execute_reply": "2023-10-11T16:06:07.994882Z",
     "shell.execute_reply.started": "2023-10-11T16:06:07.990880Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_max_length = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb73c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:06:09.082827Z",
     "iopub.status.busy": "2023-10-11T16:06:09.081870Z",
     "iopub.status.idle": "2023-10-11T16:11:57.847593Z",
     "shell.execute_reply": "2023-10-11T16:11:57.846730Z",
     "shell.execute_reply.started": "2023-10-11T16:06:09.082789Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in NLP_MODEL :\n",
    "    print(f'{model.capitalize()} ')\n",
    "    for target in [\"wording\",\"content\"]:\n",
    "        \n",
    "        if model == 'deberta_base_wording' and target =='content':continue\n",
    "        if model == 'deberta_base_content' and target =='wording':continue\n",
    "        if model == 'deberta_large_wording' and target =='content':continue\n",
    "        if model == 'deberta_large_content2fold' and target =='wording':continue\n",
    "        if model == 'deberta_large_content4fold' and target =='wording':continue\n",
    "            \n",
    "        print(f'{target} .', end='')\n",
    "        \n",
    "        if model =='debertav3base_freeze6_layers':\n",
    "            train[f'depertav3_6layer_freeze{target}_pred'] = debertav3base_freeze6_layers[f'{target}_pred'].values\n",
    "            test_max_length=384\n",
    "            \n",
    "        if model == 'deberta_base_wording' and target =='wording':\n",
    "            train['wording_pred-deberta-v3-base'] = deberta_base_wording['wording_pred'].values\n",
    "            test_max_length=448\n",
    "            \n",
    "        if model == 'deberta_base_content' and target =='content':\n",
    "            train['content_pred-deberta-v3-base'] = deberta_base_content['content_pred'].values\n",
    "            test_max_length=448\n",
    "            \n",
    "        if model == 'deberta_large_wording' and target =='wording':\n",
    "            train['wording_pred_microsoft-deberta-v3-large'] = deberta_large_wording['wording_pred'].values\n",
    "            test_max_length=512\n",
    "            \n",
    "        if model == 'deberta_large_content2fold' and target =='content':\n",
    "            train['content_pred_deberta_large_content2fold'] = deberta_large_content2fold['content_pred'].values\n",
    "            test_max_length=512\n",
    "            \n",
    "        if model == 'deberta_large_content4fold' and target =='content':\n",
    "            train['content_pred_microsoft-deberta-v3-large'] = deberta_large_content4fold['content_pred'].values\n",
    "            test_max_length=512\n",
    "            \n",
    "        if model == 'google_electra_base':\n",
    "            train[f'{target}_pred_google-electra-base-discriminator'] = google_electra_base[f'{target}_pred'].values\n",
    "            test_max_length=256\n",
    "            \n",
    "        if model == 'deperta_small':\n",
    "            train[f'deperta_small_{target}_pred'] = deperta_small[f'{target}_pred'].values\n",
    "            test_max_length=256\n",
    "            \n",
    "            \n",
    "        print('..')\n",
    "        test = predict(\n",
    "            test,\n",
    "            target=target,\n",
    "            model_name=model,\n",
    "            hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "            max_length=CFG.max_length,\n",
    "            test_max_length=test_max_length\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831b177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:47.541155Z",
     "iopub.status.busy": "2023-10-11T16:12:47.540891Z",
     "iopub.status.idle": "2023-10-11T16:12:47.547424Z",
     "shell.execute_reply": "2023-10-11T16:12:47.546271Z",
     "shell.execute_reply.started": "2023-10-11T16:12:47.541127Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2826c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:48.388027Z",
     "iopub.status.busy": "2023-10-11T16:12:48.387065Z",
     "iopub.status.idle": "2023-10-11T16:12:48.394973Z",
     "shell.execute_reply": "2023-10-11T16:12:48.394043Z",
     "shell.execute_reply.started": "2023-10-11T16:12:48.387973Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.columns, test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318af78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337d4d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:49.230896Z",
     "iopub.status.busy": "2023-10-11T16:12:49.230649Z",
     "iopub.status.idle": "2023-10-11T16:12:49.236456Z",
     "shell.execute_reply": "2023-10-11T16:12:49.235187Z",
     "shell.execute_reply.started": "2023-10-11T16:12:49.230869Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\"\n",
    "#                 'length_ratio',\n",
    "               ] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457f9ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:49.835274Z",
     "iopub.status.busy": "2023-10-11T16:12:49.835004Z",
     "iopub.status.idle": "2023-10-11T16:12:51.211381Z",
     "shell.execute_reply": "2023-10-11T16:12:51.210444Z",
     "shell.execute_reply.started": "2023-10-11T16:12:49.835222Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(CFG.n_splits):\n",
    "\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.040, # 0.048\n",
    "            'max_depth': 3, #3\n",
    "            'lambda_l1': 0.0,\n",
    "            'lambda_l2': 0.011,\n",
    "            'verbosity':-1,\n",
    "        }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        model = lgb.train(params,\n",
    "                          num_boost_round=10000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                               lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                            ],\n",
    "                          )\n",
    "        models.append(model)\n",
    "    \n",
    "    model_dict[target] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d70941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:51.213783Z",
     "iopub.status.busy": "2023-10-11T16:12:51.213076Z",
     "iopub.status.idle": "2023-10-11T16:12:51.349975Z",
     "shell.execute_reply": "2023-10-11T16:12:51.349060Z",
     "shell.execute_reply.started": "2023-10-11T16:12:51.213748Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfde73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:51.352092Z",
     "iopub.status.busy": "2023-10-11T16:12:51.351621Z",
     "iopub.status.idle": "2023-10-11T16:12:51.357948Z",
     "shell.execute_reply": "2023-10-11T16:12:51.357085Z",
     "shell.execute_reply.started": "2023-10-11T16:12:51.352058Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "                #\"fold\", \n",
    "                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\",\n",
    "                \"input\"\n",
    "#                 \"length_ratio\"\n",
    "               ] + [\n",
    "                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n",
    "                ] + [\n",
    "                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb6bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:52.202303Z",
     "iopub.status.busy": "2023-10-11T16:12:52.201708Z",
     "iopub.status.idle": "2023-10-11T16:12:52.224536Z",
     "shell.execute_reply": "2023-10-11T16:12:52.223721Z",
     "shell.execute_reply.started": "2023-10-11T16:12:52.202267Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "    preds = []\n",
    "\n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = test.drop(columns=drop_columns)\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    pred_dict[target] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93aaff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T16:12:52.721522Z",
     "iopub.status.busy": "2023-10-11T16:12:52.721217Z",
     "iopub.status.idle": "2023-10-11T16:12:52.739626Z",
     "shell.execute_reply": "2023-10-11T16:12:52.738447Z",
     "shell.execute_reply.started": "2023-10-11T16:12:52.721494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    preds = pred_dict[target]\n",
    "    for i, pred in enumerate(preds):\n",
    "        test[f\"{target}_pred_{i}\"] = pred\n",
    "    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n",
    "#     test[target+'_lgbm'] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n",
    "\n",
    "test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46c89c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "*****************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-11T16:14:08.994181",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}