{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-04T16:28:18.492697Z","iopub.execute_input":"2023-10-04T16:28:18.493119Z","iopub.status.idle":"2023-10-04T16:29:27.478968Z","shell.execute_reply.started":"2023-10-04T16:28:18.493073Z","shell.execute_reply":"2023-10-04T16:29:27.477826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModelForMultipleChoice\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\n\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nfrom cuml.svm import SVR\nimport cuml\nfrom sklearn.metrics import mean_squared_error\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:27.481186Z","iopub.execute_input":"2023-10-04T16:29:27.481811Z","iopub.status.idle":"2023-10-04T16:29:49.898176Z","shell.execute_reply.started":"2023-10-04T16:29:27.481775Z","shell.execute_reply":"2023-10-04T16:29:49.897295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:49.899530Z","iopub.execute_input":"2023-10-04T16:29:49.899768Z","iopub.status.idle":"2023-10-04T16:29:49.911828Z","shell.execute_reply.started":"2023-10-04T16:29:49.899737Z","shell.execute_reply":"2023-10-04T16:29:49.910876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name=\"debertav3base\" # roberta-base\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.00\n    attention_probs_dropout_prob=0.00\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    random_seed=42\n    save_steps=100\n    max_length=512\n    test_max_length=382","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:49.914585Z","iopub.execute_input":"2023-10-04T16:29:49.915091Z","iopub.status.idle":"2023-10-04T16:29:49.920552Z","shell.execute_reply.started":"2023-10-04T16:29:49.915059Z","shell.execute_reply":"2023-10-04T16:29:49.919727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n\nsummaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")#[4:10]\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\nsample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:49.921825Z","iopub.execute_input":"2023-10-04T16:29:49.922672Z","iopub.status.idle":"2023-10-04T16:29:50.038503Z","shell.execute_reply.started":"2023-10-04T16:29:49.922640Z","shell.execute_reply":"2023-10-04T16:29:50.037630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int:\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]:\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        \n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        \n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x)\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n#         input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        \n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:50.039811Z","iopub.execute_input":"2023-10-04T16:29:50.040261Z","iopub.status.idle":"2023-10-04T16:29:52.139457Z","shell.execute_reply.started":"2023-10-04T16:29:50.040212Z","shell.execute_reply":"2023-10-04T16:29:52.138443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = preprocessor.run(prompts_train, summaries_train, mode=\"train\")\ntest = preprocessor.run(prompts_test, summaries_test, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:52.140912Z","iopub.execute_input":"2023-10-04T16:29:52.141400Z","iopub.status.idle":"2023-10-04T16:29:53.051592Z","shell.execute_reply.started":"2023-10-04T16:29:52.141366Z","shell.execute_reply":"2023-10-04T16:29:53.050757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits = CFG.n_splits)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.053010Z","iopub.execute_input":"2023-10-04T16:29:53.053494Z","iopub.status.idle":"2023-10-04T16:29:53.063547Z","shell.execute_reply.started":"2023-10-04T16:29:53.053459Z","shell.execute_reply":"2023-10-04T16:29:53.062679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.065146Z","iopub.execute_input":"2023-10-04T16:29:53.065786Z","iopub.status.idle":"2023-10-04T16:29:53.074593Z","shell.execute_reply.started":"2023-10-04T16:29:53.065743Z","shell.execute_reply":"2023-10-04T16:29:53.073514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.078524Z","iopub.execute_input":"2023-10-04T16:29:53.079371Z","iopub.status.idle":"2023-10-04T16:29:53.086073Z","shell.execute_reply.started":"2023-10-04T16:29:53.079313Z","shell.execute_reply":"2023-10-04T16:29:53.084994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.087518Z","iopub.execute_input":"2023-10-04T16:29:53.087756Z","iopub.status.idle":"2023-10-04T16:29:53.099510Z","shell.execute_reply.started":"2023-10-04T16:29:53.087726Z","shell.execute_reply":"2023-10-04T16:29:53.098576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.100718Z","iopub.execute_input":"2023-10-04T16:29:53.101490Z","iopub.status.idle":"2023-10-04T16:29:53.109627Z","shell.execute_reply.started":"2023-10-04T16:29:53.101460Z","shell.execute_reply":"2023-10-04T16:29:53.108774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                test_max_length: int,\n                ):\n        \n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        self.test_max_length = test_max_length\n        \n        \n        \n        if TRANFORMERS_MODEL:\n            model_name = f'transformers/{model_name}'\n         \n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.test_max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep \n                    + train_df[\"prompt_question\"] + sep \n                    + train_df[\"fixed_summary_text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep \n                    + valid_df[\"prompt_question\"] + sep \n                    + valid_df[\"fixed_summary_text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config\n        )\n        \n        print('Freezing embeddings.')\n        model_content.base_model.embeddings.requires_grad_(False)\n        print(f'Freezing {FREEZE_LAYERS} layers.')\n        for k, param in model_content.base_model.encoder.layer.named_parameters():\n            l = int(k.split(\".\")[0])\n            if l < FREEZE_LAYERS:\n                param.requires_grad = False\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep \n                    + test_df[\"prompt_question\"] + sep \n                    + test_df[\"fixed_summary_text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n\n        if TRANFORMERS_MODEL:\n            model_fold_dir = self.model_dir\n        \n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.111184Z","iopub.execute_input":"2023-10-04T16:29:53.111754Z","iopub.status.idle":"2023-10-04T16:29:53.131283Z","shell.execute_reply.started":"2023-10-04T16:29:53.111722Z","shell.execute_reply":"2023-10-04T16:29:53.130291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int,\n        test_max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test_max_length= test_max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int,\n    test_max_length: int,\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"/kaggle/input/commonlite{model_name}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"/kaggle/input/commonlite{model_name}/{model_name}/fold_{fold}\"\n            \n        if WORDING_MODEL:\n            model_dir =  f\"/kaggle/input/commonlite{model_name}-wording/{target}/{model_name}/fold_{fold}\"\n        \n#         if ELECTRA_MODEL:\n#             model_dir =  f\"/kaggle/input/commonlite{model_name}-wording/{target}/{model_name}/fold_{fold}\"\n            \n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test_max_length=test_max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred_{model_name}\"] = pred\n\n    return train_df\n    \ndef predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int,\n    test_max_length:int,\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"/kaggle/input/commonlite{model_name}/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"/kaggle/input/commonlite{model_name}/{target}/{model_name}/fold_{fold}\"\n\n        if WORDING_MODEL:\n            model_dir =  f\"/kaggle/input/commonlite{model_name}-wording/{target}/{model_name}/fold_{fold}\"\n            \n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n            test_max_length=test_max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df,\n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}_{model_name}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.132587Z","iopub.execute_input":"2023-10-04T16:29:53.133016Z","iopub.status.idle":"2023-10-04T16:29:53.150699Z","shell.execute_reply.started":"2023-10-04T16:29:53.132978Z","shell.execute_reply":"2023-10-04T16:29:53.149806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_PEFT = False\nFREEZE_LAYERS = 6\n# BOOLEAN TO FREEZE EMBEDDINGS\nFREEZE_EMBEDDINGS = True\n\nWORDING_MODEL = False\nTRANFORMERS_MODEL = False","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.151851Z","iopub.execute_input":"2023-10-04T16:29:53.152703Z","iopub.status.idle":"2023-10-04T16:29:53.167790Z","shell.execute_reply.started":"2023-10-04T16:29:53.152674Z","shell.execute_reply":"2023-10-04T16:29:53.166942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking Models","metadata":{}},{"cell_type":"code","source":"stack_model = [\n    'bert-large-uncased',\n    'roberta-large',\n    'google-electra-base-discriminator',\n    'microsoft-deberta-v3-large',\n    'debertav3base', \n    'roberta-base',\n    'debertav3small'\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.169206Z","iopub.execute_input":"2023-10-04T16:29:53.169729Z","iopub.status.idle":"2023-10-04T16:29:53.179714Z","shell.execute_reply.started":"2023-10-04T16:29:53.169701Z","shell.execute_reply":"2023-10-04T16:29:53.178895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in stack_model :\n    print('*'*25,model.capitalize(),'*'*25,)\n    for target in [\"wording\",\"content\"]:\n        # there are to model for deberta large on for content and other for wording\n        WORDING_MODEL = False\n        if model =='microsoft-deberta-v3-large' and target =='wording':\n            WORDING_MODEL = True\n            \n        # used other transfomer model -noteDeberta model-\n        TRANFORMERS_MODEL = False\n        if model == 'google-electra-base-discriminator':\n            TRANFORMERS_MODEL = True\n        if model == 'roberta-large':\n            TRANFORMERS_MODEL = True    \n        if model == 'bert-large-uncased':\n            TRANFORMERS_MODEL = True\n            \n        # only content for bert model\n        if model =='bert-large-uncased' and target =='wording':\n            continue\n            \n        print('$'*10,'Inference Training',target.capitalize())\n        train = validate(\n            train,\n            target=target,\n            save_each_model=True,\n            model_name=model,\n            hidden_dropout_prob=CFG.hidden_dropout_prob,\n            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n            max_length=CFG.max_length,\n            test_max_length=CFG.test_max_length\n        )\n\n        rmse = mean_squared_error(train[target], train[f\"{target}_pred_{model}\"], squared=False)\n        print(f\"rmse: {rmse:.4f}\")\n        print('$'*10,'Inference Test')\n        test = predict(\n            test,\n            target=target,\n            save_each_model=True,\n            model_name=model,\n            hidden_dropout_prob=CFG.hidden_dropout_prob,\n            attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n            max_length=CFG.max_length,\n            test_max_length=CFG.test_max_length\n        )","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:29:53.180803Z","iopub.execute_input":"2023-10-04T16:29:53.181525Z","iopub.status.idle":"2023-10-04T16:43:54.777059Z","shell.execute_reply.started":"2023-10-04T16:29:53.181486Z","shell.execute_reply":"2023-10-04T16:43:54.775190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:45:55.770552Z","iopub.execute_input":"2023-10-04T16:45:55.770791Z","iopub.status.idle":"2023-10-04T16:45:55.776691Z","shell.execute_reply.started":"2023-10-04T16:45:55.770764Z","shell.execute_reply":"2023-10-04T16:45:55.775866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns, test.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:45:57.384407Z","iopub.execute_input":"2023-10-04T16:45:57.384661Z","iopub.status.idle":"2023-10-04T16:45:57.391062Z","shell.execute_reply.started":"2023-10-04T16:45:57.384633Z","shell.execute_reply":"2023-10-04T16:45:57.390182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Transformer Models","metadata":{"execution":{"iopub.status.busy":"2023-09-30T18:59:45.946524Z","iopub.execute_input":"2023-09-30T18:59:45.947118Z","iopub.status.idle":"2023-09-30T18:59:45.951424Z","shell.execute_reply.started":"2023-09-30T18:59:45.947084Z","shell.execute_reply":"2023-09-30T18:59:45.950177Z"}}},{"cell_type":"code","source":"contents = \\\n0.05 * test[f\"content_bert-large-uncased\"] +\\\n0.15 * test[f\"content_roberta-large\"] +\\\n0.20 * test[f\"content_microsoft-deberta-v3-large\"] +\\\n0.20 * test[f\"content_google-electra-base-discriminator\"] +\\\n0.20 * test[f\"content_debertav3base\"] +\\\n0.15 * test[f\"content_roberta-base\"] +\\\n0.05 * test[f\"content_debertav3small\"]\n\n# 0.10 * test[f\"wording_bert-large-uncased\"] +\\\nwordings =\\\n0.15 * test[f\"wording_roberta-large\"] +\\\n0.20 * test[f\"wording_microsoft-deberta-v3-large\"] +\\\n0.20 * test[f\"wording_google-electra-base-discriminator\"] +\\\n0.20 * test[f\"wording_debertav3base\"] +\\\n0.20 * test[f\"wording_roberta-base\"] + \\\n0.05 * test[f\"wording_debertav3small\"]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:46:58.756128Z","iopub.execute_input":"2023-10-04T16:46:58.756995Z","iopub.status.idle":"2023-10-04T16:46:58.765502Z","shell.execute_reply.started":"2023-10-04T16:46:58.756946Z","shell.execute_reply":"2023-10-04T16:46:58.764483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test['content'] = contents\n# test['wording']= wordings\n\n# test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:01.285659Z","iopub.execute_input":"2023-10-04T16:47:01.285919Z","iopub.status.idle":"2023-10-04T16:47:01.290687Z","shell.execute_reply.started":"2023-10-04T16:47:01.285894Z","shell.execute_reply":"2023-10-04T16:47:01.289514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LIGHTGBM","metadata":{}},{"cell_type":"code","source":"targets = [\"content\", \"wording\"]\n\ndrop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\"\n#                 'length_ratio',\n               ] + targets","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:02.470790Z","iopub.execute_input":"2023-10-04T16:47:02.471689Z","iopub.status.idle":"2023-10-04T16:47:02.476866Z","shell.execute_reply.started":"2023-10-04T16:47:02.471651Z","shell.execute_reply":"2023-10-04T16:47:02.476030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = {}\n\nfor target in targets:\n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n\n        params = {\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'objective': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.040, # 0.048\n            'max_depth': 5, #3\n            'lambda_l1': 0.0,\n            'lambda_l2': 0.011,\n            'verbosity':-1,\n        }\n\n        evaluation_results = {}\n        model = lgb.train(params,\n                          num_boost_round=10000,\n                            #categorical_feature = categorical_features,\n                          valid_names=['train', 'valid'],\n                          train_set=dtrain,\n                          valid_sets=dval,\n                          callbacks=[\n                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n                               lgb.log_evaluation(100),\n                              lgb.callback.record_evaluation(evaluation_results)\n                            ],\n                          )\n        models.append(model)\n    \n    model_dict[target] = models","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:03.448381Z","iopub.execute_input":"2023-10-04T16:47:03.448628Z","iopub.status.idle":"2023-10-04T16:47:03.590788Z","shell.execute_reply.started":"2023-10-04T16:47:03.448603Z","shell.execute_reply":"2023-10-04T16:47:03.589910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM CV","metadata":{}},{"cell_type":"code","source":"rmses = []\n\nfor target in targets:\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")\n# content_rmse : 0.50138\n# wording_rmse : 0.66961\n# mcrmse : 0.58550","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:08.136587Z","iopub.execute_input":"2023-10-04T16:47:08.137362Z","iopub.status.idle":"2023-10-04T16:47:08.195894Z","shell.execute_reply.started":"2023-10-04T16:47:08.137321Z","shell.execute_reply":"2023-10-04T16:47:08.195177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM PREDS","metadata":{"execution":{"iopub.status.busy":"2023-09-23T20:52:57.755551Z","iopub.execute_input":"2023-09-23T20:52:57.756091Z","iopub.status.idle":"2023-09-23T20:52:57.793814Z","shell.execute_reply.started":"2023-09-23T20:52:57.756058Z","shell.execute_reply":"2023-09-23T20:52:57.792678Z"}}},{"cell_type":"code","source":"drop_columns = [\n                #\"fold\", \n                \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n                \"prompt_question\", \"prompt_title\", \n                \"prompt_text\",\n                \"input\"\n#                 \"length_ratio\"\n               ] + [\n                f\"content_pred_{i}\" for i in range(CFG.n_splits)\n                ] + [\n                f\"wording_pred_{i}\" for i in range(CFG.n_splits)\n                ]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:09.347920Z","iopub.execute_input":"2023-10-04T16:47:09.348841Z","iopub.status.idle":"2023-10-04T16:47:09.354259Z","shell.execute_reply.started":"2023-10-04T16:47:09.348790Z","shell.execute_reply":"2023-10-04T16:47:09.353371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test.drop(columns=drop_columns)\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:10.524919Z","iopub.execute_input":"2023-10-04T16:47:10.525588Z","iopub.status.idle":"2023-10-04T16:47:10.548180Z","shell.execute_reply.started":"2023-10-04T16:47:10.525547Z","shell.execute_reply":"2023-10-04T16:47:10.547416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n#     test[target+'_lgbm'] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n# test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)\n\nwordings = wordings * 0.10 + test['wording'] * 0.90\ncontents = contents * 0.10 + test['content'] * 0.90\n\ntest['content'] = contents\ntest['wording']= wordings\ntest[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T16:47:18.791051Z","iopub.execute_input":"2023-10-04T16:47:18.791844Z","iopub.status.idle":"2023-10-04T16:47:18.819555Z","shell.execute_reply.started":"2023-10-04T16:47:18.791799Z","shell.execute_reply":"2023-10-04T16:47:18.818458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVC","metadata":{}},{"cell_type":"code","source":"# # SVM\n# TEST_FEATURE = []\n# TRAIN_FEATURE = []\n# for model in ['microsoft-deberta-v3-large', \"debertav3base\", \"roberta-base\"]:\n#     for target in [\"content\", \"wording\"]:\n#         if model =='microsoft-deberta-v3-large' and target =='wording':continue\n#         TRAIN_FEATURE.append(f\"{target}_pred_{model}\")\n#         TEST_FEATURE.append(f\"{target}_{model}\")\n\n# model_dict = {}\n# for target in targets:\n#     models = []\n    \n#     for fold in range(CFG.n_splits):\n# #         print(f'fold{fold}', end=',')\n#         X_train_cv = train[train[\"fold\"] != fold][TRAIN_FEATURE].values\n#         y_train_cv = train[train[\"fold\"] != fold][target]\n\n        \n#         X_eval_cv = train[train[\"fold\"] == fold][TRAIN_FEATURE]\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         clf = SVR(C=5)\n#         clf.fit(X_train_cv, y_train_cv)\n\n#         models.append(clf)\n    \n#     model_dict[target] = models","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:33:59.274402Z","iopub.execute_input":"2023-09-25T19:33:59.275224Z","iopub.status.idle":"2023-09-25T19:33:59.565017Z","shell.execute_reply.started":"2023-09-25T19:33:59.275185Z","shell.execute_reply":"2023-09-25T19:33:59.564064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVR CV Score","metadata":{}},{"cell_type":"code","source":"# rmses = []\n\n# for target in targets:\n#     models = model_dict[target]\n\n#     preds = []\n#     trues = []\n    \n#     for fold, model in enumerate(models):\n#         X_eval_cv = train[train[\"fold\"] == fold][TRAIN_FEATURE].values\n#         y_eval_cv = train[train[\"fold\"] == fold][target]\n\n#         pred = model.predict(X_eval_cv)\n\n#         trues.extend(y_eval_cv)\n#         preds.extend(pred)\n        \n#     rmse = np.sqrt(mean_squared_error(trues, preds))\n#     print(f\"{target}_rmse : {rmse}\")\n#     rmses = rmses + [rmse]\n\n# print(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:34:11.505794Z","iopub.execute_input":"2023-09-25T19:34:11.506133Z","iopub.status.idle":"2023-09-25T19:34:11.556140Z","shell.execute_reply.started":"2023-09-25T19:34:11.506099Z","shell.execute_reply":"2023-09-25T19:34:11.555164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_dict = {}\n# for target in targets:\n#     models = model_dict[target]\n#     preds = []\n\n#     for fold, model in enumerate(models):\n#         X_eval_cv = test[TEST_FEATURE].values\n\n#         pred = model.predict(X_eval_cv)\n#         preds.append(pred)\n    \n#     pred_dict[target] = preds\n\n# for target in targets:\n#     preds = pred_dict[target]\n#     for i, pred in enumerate(preds):\n#         test[f\"{target}_pred_{i}\"] = pred\n#     test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n#     test[target+'_svr'] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n    \n# # test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:34:13.104318Z","iopub.execute_input":"2023-09-25T19:34:13.104930Z","iopub.status.idle":"2023-09-25T19:34:13.140462Z","shell.execute_reply.started":"2023-09-25T19:34:13.104879Z","shell.execute_reply":"2023-09-25T19:34:13.139528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ensemble SVR + LGBM","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:24:33.905185Z","iopub.execute_input":"2023-09-25T19:24:33.905461Z","iopub.status.idle":"2023-09-25T19:24:33.915593Z","shell.execute_reply.started":"2023-09-25T19:24:33.905427Z","shell.execute_reply":"2023-09-25T19:24:33.913298Z"}}},{"cell_type":"code","source":"# lgb_w = 0.8\n# svr_w = 0.2\n\n# for target in targets:\n#     test[target] = test[target+'_lgbm'] * lgb_w + test[target+'_svr'] * svr_w\n    \n# test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T19:34:15.335296Z","iopub.execute_input":"2023-09-25T19:34:15.335913Z","iopub.status.idle":"2023-09-25T19:34:15.346418Z","shell.execute_reply.started":"2023-09-25T19:34:15.335868Z","shell.execute_reply":"2023-09-25T19:34:15.345398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DON'T FORGET TRAIN SIZE","metadata":{}}]}